"""
æµ‹è¯•ç»“æ„åŒ–æ—¥å¿—ç³»ç»Ÿ

éªŒè¯JSON+MarkdownåŒå†™åŠŸèƒ½ï¼š
- ä¸åŒæ—¥å¿—çº§åˆ«çš„è¾“å‡º
- æ™ºèƒ½ä½“è¾“å‡ºçš„ç»“æ„åŒ–è®°å½•
- æµæ°´çº¿é˜¶æ®µçš„è¿½è¸ª
- å†³ç­–è¿‡ç¨‹çš„è®°å½•
- å¼‚æ­¥å†™å…¥æ€§èƒ½
"""

import unittest
import tempfile
import shutil
import json
import time
from pathlib import Path
from datetime import datetime
from unittest.mock import Mock

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

from mytrade.logging.structured_logger import (
    DualFormatLogger, StructuredLogLevel, LogCategory,
    get_structured_logger, close_structured_logger,
    log_debug, log_info, log_analysis
)
from mytrade.agents.protocols import (
    AgentRole, AgentOutput, AgentDecision, DecisionAction, AgentMetadata
)


class TestStructuredLogger(unittest.TestCase):
    """æµ‹è¯•ç»“æ„åŒ–æ—¥å¿—è®°å½•å™¨"""
    
    def setUp(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        self.test_dir = Path(tempfile.mkdtemp())
        self.session_id = "test_session_001"
        
        self.logger = DualFormatLogger(
            log_dir=str(self.test_dir),
            session_id=self.session_id,
            enable_json=True,
            enable_markdown=True,
            enable_console=False,  # æµ‹è¯•æ—¶ç¦ç”¨æ§åˆ¶å°è¾“å‡º
            async_mode=False  # åŒæ­¥æ¨¡å¼ä¾¿äºæµ‹è¯•éªŒè¯
        )
    
    def tearDown(self):
        """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        self.logger.close()
        shutil.rmtree(self.test_dir)
        close_structured_logger()  # æ¸…ç†å…¨å±€å®ä¾‹
    
    def test_basic_logging(self):
        """æµ‹è¯•åŸºæœ¬æ—¥å¿—è®°å½•åŠŸèƒ½"""
        # è®°å½•ä¸åŒçº§åˆ«çš„æ—¥å¿—
        self.logger.log(
            level=StructuredLogLevel.INFO,
            category=LogCategory.SYSTEM,
            component="test_component",
            message="æµ‹è¯•æ¶ˆæ¯",
            data={"key": "value", "number": 42}
        )
        
        self.logger.log(
            level=StructuredLogLevel.WARNING,
            category=LogCategory.AGENT,
            component="agent_test",
            message="è­¦å‘Šæ¶ˆæ¯",
            data={"warning_type": "performance"}
        )
        
        # éªŒè¯JSONæ–‡ä»¶ç”Ÿæˆ
        json_files = list(self.test_dir.glob(f"session_{self.session_id}_*.json"))
        self.assertEqual(len(json_files), 1)
        
        # éªŒè¯Markdownæ–‡ä»¶ç”Ÿæˆ
        md_files = list(self.test_dir.glob(f"session_{self.session_id}_*.md"))
        self.assertEqual(len(md_files), 1)
        
        # éªŒè¯JSONå†…å®¹
        with open(json_files[0], 'r', encoding='utf-8') as f:
            lines = f.readlines()
            self.assertEqual(len(lines), 2)  # ä¸¤æ¡æ—¥å¿—
            
            # è§£æç¬¬ä¸€æ¡æ—¥å¿—
            log_entry1 = json.loads(lines[0])
            self.assertEqual(log_entry1['level'], 'info')
            self.assertEqual(log_entry1['category'], 'system')
            self.assertEqual(log_entry1['component'], 'test_component')
            self.assertEqual(log_entry1['message'], 'æµ‹è¯•æ¶ˆæ¯')
            self.assertEqual(log_entry1['data']['key'], 'value')
            self.assertEqual(log_entry1['data']['number'], 42)
            
        # éªŒè¯Markdownå†…å®¹
        with open(md_files[0], 'r', encoding='utf-8') as f:
            content = f.read()
            self.assertIn('# TradingAgents ç»“æ„åŒ–æ—¥å¿—', content)
            self.assertIn('INFO - test_component', content)
            self.assertIn('WARNING - agent_test', content)
            self.assertIn('æµ‹è¯•æ¶ˆæ¯', content)
            self.assertIn('è­¦å‘Šæ¶ˆæ¯', content)
    
    def test_agent_output_logging(self):
        """æµ‹è¯•æ™ºèƒ½ä½“è¾“å‡ºè®°å½•"""
        # åˆ›å»ºæ¨¡æ‹Ÿçš„æ™ºèƒ½ä½“è¾“å‡º
        agent_output = AgentOutput(
            role=AgentRole.FUNDAMENTAL,
            timestamp=datetime.now(),
            symbol="000001.SZ",
            score=0.75,
            confidence=0.82,
            decision=AgentDecision(
                action=DecisionAction.BUY,
                weight=0.15,
                confidence=0.78,
                reasoning="åŸºæœ¬é¢åˆ†ææ˜¾ç¤ºå…¬å¸ä»·å€¼è¢«ä½ä¼°",
                risk_level="medium",
                expected_return=0.12,
                max_loss=0.08
            ),
            features={
                "pe_ratio": 15.6,
                "pb_ratio": 1.8,
                "debt_ratio": 0.32
            },
            rationale="è´¢åŠ¡æŒ‡æ ‡å¥åº·ï¼Œä¸šç»©å¢é•¿ç¨³å®š",
            metadata=AgentMetadata(
                agent_id="fundamental_analyst_001",
                version="2.0.0",
                execution_time_ms=1250
            ),
            alerts=["å…³æ³¨è¡Œä¸šæ”¿ç­–é£é™©"],
            tags=["value_stock", "defensive"]
        )
        
        # è®°å½•æ™ºèƒ½ä½“è¾“å‡º
        self.logger.log_agent_output(agent_output, "fundamental_analyst")
        
        # éªŒè¯JSONè®°å½•
        json_files = list(self.test_dir.glob(f"session_{self.session_id}_*.json"))
        with open(json_files[0], 'r', encoding='utf-8') as f:
            log_entry = json.loads(f.readline())
            
            self.assertEqual(log_entry['level'], 'analysis')
            self.assertEqual(log_entry['category'], 'agent')
            self.assertEqual(log_entry['component'], 'fundamental_analyst.fundamental')
            self.assertEqual(log_entry['data']['symbol'], '000001.SZ')
            self.assertEqual(log_entry['data']['score'], 0.75)
            self.assertEqual(log_entry['data']['confidence'], 0.82)
            self.assertEqual(log_entry['data']['decision']['action'], 'buy')
            self.assertEqual(log_entry['data']['features']['pe_ratio'], 15.6)
            self.assertIn('åŸºæœ¬é¢åˆ†ææ˜¾ç¤ºå…¬å¸ä»·å€¼è¢«ä½ä¼°', log_entry['data']['decision']['reasoning'])
    
    def test_pipeline_stage_logging(self):
        """æµ‹è¯•æµæ°´çº¿é˜¶æ®µè®°å½•"""
        stage_results = {
            "analysts_count": 3,
            "success_rate": 0.67,
            "avg_confidence": 0.75,
            "consensus_score": 0.68
        }
        
        self.logger.log_pipeline_stage(
            stage="analysis",
            status="completed",
            results=stage_results,
            duration_ms=2150
        )
        
        # éªŒè¯æ—¥å¿—è®°å½•
        json_files = list(self.test_dir.glob(f"session_{self.session_id}_*.json"))
        with open(json_files[0], 'r', encoding='utf-8') as f:
            log_entry = json.loads(f.readline())
            
            self.assertEqual(log_entry['level'], 'info')
            self.assertEqual(log_entry['category'], 'pipeline')
            self.assertEqual(log_entry['component'], 'orchestrator')
            self.assertEqual(log_entry['data']['stage'], 'analysis')
            self.assertEqual(log_entry['data']['status'], 'completed')
            self.assertEqual(log_entry['data']['duration_ms'], 2150)
            self.assertEqual(log_entry['data']['results']['consensus_score'], 0.68)
    
    def test_decision_logging(self):
        """æµ‹è¯•å†³ç­–è®°å½•"""
        decision = AgentDecision(
            action=DecisionAction.BUY,
            weight=0.12,
            confidence=0.85,
            reasoning="å¤šé¡¹åˆ†ææŒ‡æ ‡æ”¯æŒä¹°å…¥å†³ç­–",
            risk_level="medium",
            expected_return=0.15,
            max_loss=0.06,
            time_horizon="3M"
        )
        
        self.logger.log_decision(
            decision=decision,
            context="ç»¼åˆåˆ†æåçš„æœ€ç»ˆäº¤æ˜“å†³ç­–",
            component="trading_engine"
        )
        
        # éªŒè¯å†³ç­–æ—¥å¿—
        json_files = list(self.test_dir.glob(f"session_{self.session_id}_*.json"))
        with open(json_files[0], 'r', encoding='utf-8') as f:
            log_entry = json.loads(f.readline())
            
            self.assertEqual(log_entry['level'], 'decision')
            self.assertEqual(log_entry['category'], 'trading')
            self.assertEqual(log_entry['component'], 'trading_engine')
            self.assertEqual(log_entry['data']['action'], 'buy')
            self.assertEqual(log_entry['data']['weight'], 0.12)
            self.assertEqual(log_entry['data']['confidence'], 0.85)
            self.assertEqual(log_entry['data']['time_horizon'], '3M')
    
    def test_error_logging(self):
        """æµ‹è¯•é”™è¯¯è®°å½•"""
        try:
            # æ•…æ„åˆ¶é€ å¼‚å¸¸
            raise ValueError("æµ‹è¯•å¼‚å¸¸æƒ…å†µ")
        except Exception as e:
            self.logger.log_error(
                component="test_component",
                error=e,
                context={"operation": "test_error_handling", "input_data": {"symbol": "000001"}}
            )
        
        # éªŒè¯é”™è¯¯æ—¥å¿—
        json_files = list(self.test_dir.glob(f"session_{self.session_id}_*.json"))
        with open(json_files[0], 'r', encoding='utf-8') as f:
            log_entry = json.loads(f.readline())
            
            self.assertEqual(log_entry['level'], 'error')
            self.assertEqual(log_entry['category'], 'system')
            self.assertEqual(log_entry['component'], 'test_component')
            self.assertEqual(log_entry['data']['error_type'], 'ValueError')
            self.assertEqual(log_entry['data']['error_message'], 'æµ‹è¯•å¼‚å¸¸æƒ…å†µ')
            self.assertIn('traceback', log_entry['data'])
            self.assertEqual(log_entry['data']['context']['operation'], 'test_error_handling')
    
    def test_performance_logging(self):
        """æµ‹è¯•æ€§èƒ½æŒ‡æ ‡è®°å½•"""
        self.logger.log_performance(
            component="data_fetcher",
            metric="response_time",
            value=156.7,
            unit="ms",
            additional_data={
                "endpoint": "/api/stock/000001",
                "cache_hit": False,
                "data_size": 2048
            }
        )
        
        # éªŒè¯æ€§èƒ½æ—¥å¿—
        json_files = list(self.test_dir.glob(f"session_{self.session_id}_*.json"))
        with open(json_files[0], 'r', encoding='utf-8') as f:
            log_entry = json.loads(f.readline())
            
            self.assertEqual(log_entry['level'], 'info')
            self.assertEqual(log_entry['category'], 'performance')
            self.assertEqual(log_entry['component'], 'data_fetcher')
            self.assertEqual(log_entry['data']['metric'], 'response_time')
            self.assertEqual(log_entry['data']['value'], 156.7)
            self.assertEqual(log_entry['data']['unit'], 'ms')
            self.assertEqual(log_entry['data']['cache_hit'], False)
    
    def test_async_mode(self):
        """æµ‹è¯•å¼‚æ­¥æ¨¡å¼"""
        # åˆ›å»ºå¼‚æ­¥æ¨¡å¼çš„æ—¥å¿—è®°å½•å™¨
        async_logger = DualFormatLogger(
            log_dir=str(self.test_dir / "async"),
            session_id="async_test",
            async_mode=True,
            enable_console=False
        )
        
        try:
            # å¿«é€Ÿå†™å…¥å¤šæ¡æ—¥å¿—
            start_time = time.time()
            for i in range(100):
                async_logger.log(
                    level=StructuredLogLevel.INFO,
                    category=LogCategory.SYSTEM,
                    component="async_test",
                    message=f"å¼‚æ­¥æ—¥å¿—æ¶ˆæ¯ {i}",
                    data={"index": i, "batch": "performance_test"}
                )
            
            # å¼‚æ­¥æ¨¡å¼åº”è¯¥å¾ˆå¿«è¿”å›
            write_time = time.time() - start_time
            self.assertLess(write_time, 0.1)  # åº”è¯¥åœ¨100mså†…å®Œæˆ
            
            # ç­‰å¾…å¼‚æ­¥å†™å…¥å®Œæˆ
            time.sleep(0.5)
            
            # éªŒè¯æ‰€æœ‰æ—¥å¿—éƒ½è¢«å†™å…¥
            json_files = list((self.test_dir / "async").glob("session_async_test_*.json"))
            self.assertEqual(len(json_files), 1)
            
            with open(json_files[0], 'r', encoding='utf-8') as f:
                lines = f.readlines()
                self.assertEqual(len(lines), 100)
        
        finally:
            async_logger.close()
    
    def test_global_logger_functions(self):
        """æµ‹è¯•å…¨å±€æ—¥å¿—è®°å½•å‡½æ•°"""
        # æµ‹è¯•ä¾¿æ·å‡½æ•°
        log_debug("test_component", "è°ƒè¯•æ¶ˆæ¯", {"debug_level": "verbose"})
        log_info("test_component", "ä¿¡æ¯æ¶ˆæ¯", {"info_type": "status"})
        log_analysis("test_agent", "åˆ†æç»“æœ", {"confidence": 0.78})
        
        # è·å–å…¨å±€æ—¥å¿—è®°å½•å™¨éªŒè¯
        global_logger = get_structured_logger()
        self.assertIsNotNone(global_logger)
        
        # éªŒè¯æ—¥å¿—æ–‡ä»¶ç”Ÿæˆï¼ˆå…¨å±€è®°å½•å™¨ä½¿ç”¨é»˜è®¤è·¯å¾„ï¼‰
        default_log_dir = Path("logs/structured")
        if default_log_dir.exists():
            json_files = list(default_log_dir.glob("session_*.json"))
            self.assertGreater(len(json_files), 0)
    
    def test_markdown_format_quality(self):
        """æµ‹è¯•Markdownæ ¼å¼è´¨é‡"""
        # è®°å½•å¤åˆæ—¥å¿—æ¡ç›®
        self.logger.log(
            level=StructuredLogLevel.ANALYSIS,
            category=LogCategory.AGENT,
            component="technical_analyst",
            message="æŠ€æœ¯åˆ†æå®Œæˆ",
            data={
                "indicators": {
                    "sma_20": 15.67,
                    "sma_50": 14.89,
                    "rsi": 65.4,
                    "macd": 0.23
                },
                "signals": ["golden_cross", "breakout"],
                "trend": "bullish"
            },
            metadata={
                "model_version": "v2.1.0",
                "computation_time": 89.5
            }
        )
        
        # éªŒè¯Markdownæ ¼å¼
        md_files = list(self.test_dir.glob(f"session_{self.session_id}_*.md"))
        with open(md_files[0], 'r', encoding='utf-8') as f:
            content = f.read()
            
            # éªŒè¯æ ‡é¢˜å’Œç»“æ„
            self.assertIn('# TradingAgents ç»“æ„åŒ–æ—¥å¿—', content)
            self.assertIn('## ğŸ“Š ANALYSIS - technical_analyst', content)
            self.assertIn('**æ—¶é—´**:', content)
            self.assertIn('**ç±»åˆ«**: `agent`', content)
            self.assertIn('**æ¶ˆæ¯**: æŠ€æœ¯åˆ†æå®Œæˆ', content)
            
            # éªŒè¯æ•°æ®æ ¼å¼
            self.assertIn('**æ•°æ®**:', content)
            self.assertIn('```json', content)
            self.assertIn('"sma_20": 15.67', content)
            self.assertIn('"signals": ["golden_cross", "breakout"]', content)
            
            # éªŒè¯å…ƒæ•°æ®
            self.assertIn('**å…ƒæ•°æ®**:', content)
            self.assertIn('"model_version": "v2.1.0"', content)
    
    def test_context_manager_usage(self):
        """æµ‹è¯•ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç”¨æ³•"""
        test_log_dir = self.test_dir / "context_test"
        
        with DualFormatLogger(
            log_dir=str(test_log_dir),
            session_id="context_test",
            enable_console=False
        ) as logger:
            logger.log(
                level=StructuredLogLevel.INFO,
                category=LogCategory.SYSTEM,
                component="context_test",
                message="ä¸Šä¸‹æ–‡ç®¡ç†å™¨æµ‹è¯•"
            )
        
        # éªŒè¯æ–‡ä»¶æ­£å¸¸ç”Ÿæˆå’Œå…³é—­
        json_files = list(test_log_dir.glob("session_context_test_*.json"))
        md_files = list(test_log_dir.glob("session_context_test_*.md"))
        
        self.assertEqual(len(json_files), 1)
        self.assertEqual(len(md_files), 1)
        
        # éªŒè¯Markdownæ–‡ä»¶åŒ…å«ç»“æŸæ—¶é—´
        with open(md_files[0], 'r', encoding='utf-8') as f:
            content = f.read()
            self.assertIn('**ç»“æŸæ—¶é—´**:', content)


class TestStructuredLoggerIntegration(unittest.TestCase):
    """æµ‹è¯•ç»“æ„åŒ–æ—¥å¿—ç³»ç»Ÿé›†æˆ"""
    
    def test_agent_pipeline_logging_simulation(self):
        """æ¨¡æ‹Ÿæ™ºèƒ½ä½“æµæ°´çº¿çš„å®Œæ•´æ—¥å¿—è®°å½•"""
        test_dir = Path(tempfile.mkdtemp())
        
        try:
            with DualFormatLogger(
                log_dir=str(test_dir),
                session_id="pipeline_simulation",
                enable_console=False
            ) as logger:
                
                # 1. æµæ°´çº¿å¼€å§‹
                logger.log_pipeline_stage(
                    stage="initialization",
                    status="started",
                    results={"symbol": "000001.SZ", "context": "daily_analysis"}
                )
                
                # 2. åˆ†æå¸ˆé˜¶æ®µ
                analyst_outputs = []
                for role in ["fundamental", "technical", "sentiment"]:
                    agent_output = AgentOutput(
                        role=getattr(AgentRole, role.upper()),
                        timestamp=datetime.now(),
                        symbol="000001.SZ",
                        score=0.6 + (hash(role) % 40) / 100,  # æ¨¡æ‹Ÿä¸åŒåˆ†æ•°
                        confidence=0.7 + (hash(role) % 30) / 100,
                        features={f"{role}_score": 0.75, "data_quality": "high"},
                        rationale=f"{role}åˆ†ææ˜¾ç¤ºç§¯æä¿¡å·",
                        metadata=AgentMetadata(agent_id=f"{role}_agent_001")
                    )
                    analyst_outputs.append(agent_output)
                    logger.log_agent_output(agent_output, f"{role}_analyst")
                
                logger.log_pipeline_stage(
                    stage="analysis",
                    status="completed",
                    results={
                        "analysts_executed": len(analyst_outputs),
                        "avg_score": sum(o.score for o in analyst_outputs) / len(analyst_outputs),
                        "consensus": "bullish_lean"
                    },
                    duration_ms=1850
                )
                
                # 3. äº¤æ˜“å†³ç­–
                final_decision = AgentDecision(
                    action=DecisionAction.BUY,
                    weight=0.08,
                    confidence=0.73,
                    reasoning="å¤šæ•°åˆ†æå¸ˆçœ‹å¥½ï¼ŒæŠ€æœ¯æŒ‡æ ‡æ”¯æŒ",
                    risk_level="medium",
                    expected_return=0.12,
                    max_loss=0.05
                )
                
                logger.log_decision(
                    decision=final_decision,
                    context="åŸºäºä¸‰ç»´åˆ†æçš„ç»¼åˆå†³ç­–",
                    component="decision_engine"
                )
                
                # 4. æ€§èƒ½ç»Ÿè®¡
                logger.log_performance(
                    component="pipeline",
                    metric="total_execution_time",
                    value=3250,
                    unit="ms",
                    additional_data={
                        "stages_completed": 3,
                        "agents_executed": 3,
                        "decision_confidence": final_decision.confidence
                    }
                )
            
            # éªŒè¯å®Œæ•´çš„æ—¥å¿—è®°å½•
            json_files = list(test_dir.glob("session_pipeline_simulation_*.json"))
            self.assertEqual(len(json_files), 1)
            
            with open(json_files[0], 'r', encoding='utf-8') as f:
                lines = f.readlines()
                # åº”è¯¥æœ‰ï¼š1ä¸ªåˆå§‹åŒ– + 3ä¸ªåˆ†æå¸ˆ + 1ä¸ªé˜¶æ®µå®Œæˆ + 1ä¸ªå†³ç­– + 1ä¸ªæ€§èƒ½ = 7æ¡æ—¥å¿—
                self.assertEqual(len(lines), 7)
                
                # éªŒè¯æ—¥å¿—ç±»å‹åˆ†å¸ƒ
                log_levels = [json.loads(line)['level'] for line in lines]
                self.assertIn('info', log_levels)  # æµæ°´çº¿å’Œæ€§èƒ½æ—¥å¿—
                self.assertIn('analysis', log_levels)  # æ™ºèƒ½ä½“åˆ†ææ—¥å¿—  
                self.assertIn('decision', log_levels)  # å†³ç­–æ—¥å¿—
            
            # éªŒè¯Markdownæ ¼å¼çš„å¯è¯»æ€§
            md_files = list(test_dir.glob("session_pipeline_simulation_*.md"))
            with open(md_files[0], 'r', encoding='utf-8') as f:
                content = f.read()
                
                # éªŒè¯åŒ…å«æ‰€æœ‰å…³é”®ç»„ä»¶
                self.assertIn('fundamental_analyst.fundamental', content)
                self.assertIn('technical_analyst.technical', content)
                self.assertIn('sentiment_analyst.sentiment', content)
                self.assertIn('decision_engine', content)
                self.assertIn('orchestrator', content)
                
                # éªŒè¯å†³ç­–ä¿¡æ¯çš„è®°å½•
                self.assertIn('äº¤æ˜“å†³ç­–: buy', content)
                self.assertIn('åŸºäºä¸‰ç»´åˆ†æçš„ç»¼åˆå†³ç­–', content)
        
        finally:
            shutil.rmtree(test_dir)


if __name__ == '__main__':
    unittest.main()